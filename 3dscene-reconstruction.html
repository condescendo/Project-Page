<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Semantic Segmentation U-Net | Portfolio</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&family=JetBrains+Mono:wght@400;500&family=Libre+Franklin:wght@500;700;900&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="style.css">

    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-twilight.min.css" rel="stylesheet" />

    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      }
    };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

    <div class="container project-view">
        
        <header>
            <h1>Semantic<br>Segmentation</h1>
            <div class="subtitle">Dense Prediction with U-Net Architecture</div>
            
            <div class="stack-container">
                <span class="tech-tag">PyTorch</span>
                <span class="tech-tag">Computer Vision</span>
                <span class="tech-tag">NumPy</span>
                <span class="tech-tag">Matplotlib</span>
                <span class="tech-tag">Deep Learning</span>
            </div>
        </header>

        <section id="executive-summary">
            <h3>// Executive Summary</h3>
            <p>While standard object detection identifies bounding boxes, this project solves <strong>Dense Prediction</strong>: classifying every individual pixel in an image. I implemented a custom U-Net architecture from scratch in PyTorch to perform semantic segmentation. The model utilizes a fully convolutional encoder-decoder structure with skip connections to preserve high-frequency spatial details often lost during downsampling.</p>
        </section>

        <section id="phase-1">
            <h2>Phase I: The Architecture</h2>
            <p><strong>Focus:</strong> Encoder-Decoder Design & Skip Connections</p>
            <p>The U-Net architecture (Ronneberger et al.) consists of a contracting path (encoder) to capture context and a symmetric expanding path (decoder) that enables precise localization.</p>

            <div class="media-wrapper">
                <div class="diagram-placeholder">
                    
                </div>
                <div class="caption">Figure 1: U-Net Architecture (Contracting & Expanding Paths)</div>
            </div>

            <ul>
                <li><strong>The Encoder:</strong> Acts as a feature extractor. It repeatedly applies two 3x3 convolutions followed by a ReLU and a 2x2 max pooling operation for downsampling ($H_{out} = H_{in} / 2$).</li>
                <li><strong>The Bottleneck:</strong> The deepest layer of the network representing the most abstract, high-level features of the image.</li>
                <li><strong>The Decoder:</strong> Upsamples the feature map ($2 \times$ scale) and concatenates it with the corresponding feature map from the encoder (Skip Connection) to recover spatial resolution.</li>
            </ul>

            <span class="code-label">model.py / Forward Pass</span>
            <pre><code class="language-python">def forward(self, x):
    # --- Encoder (Contracting Path) ---
    x1 = self.inc(x)
    x2 = self.down1(x1)
    x3 = self.down2(x2)
    x4 = self.down3(x3)
    x5 = self.down4(x4) # Bottleneck
    
    # --- Decoder (Expanding Path with Skips) ---
    x = self.up1(x5, x4) # Concatenates x4 (skip) with upsampled x5
    x = self.up2(x, x3)
    x = self.up3(x, x2)
    x = self.up4(x, x1)
    
    # Final 1x1 Conv to map to N_Classes
    logits = self.outc(x)
    return logits</code></pre>
        </section>

        <section id="phase-2">
            <h2>Phase II: Data Pipeline</h2>
            <p><strong>Focus:</strong> Custom Datasets & Tensors</p>
            <p>Deep Learning performance relies on data quality. I implemented a custom `torch.utils.data.Dataset` class to handle image-mask pairs. Unlike classification where labels are integers, here labels are 2D segmentation masks.</p>

            <ul>
                <li><strong>Normalization:</strong> Input images are normalized to Mean $\mu=0$ and Std $\sigma=1$ to ensure stable gradient descent.</li>
                <li><strong>Augmentation:</strong> Applied random rotations and horizontal flips to reduce overfitting, ensuring the transformation applied to the image is identically applied to the mask.</li>
            </ul>

            <span class="code-label">dataset.py / Tensor Conversion</span>
            <pre><code class="language-python">def __getitem__(self, idx):
    img = load_image(self.images[idx])
    mask = load_mask(self.masks[idx])

    if self.transform:
        # Albumentations handles dual-transform of img + mask
        augmented = self.transform(image=img, mask=mask)
        img = augmented['image']
        mask = augmented['mask']

    # Permute dimensions: (H, W, C) -> (C, H, W) for PyTorch
    return img.permute(2, 0, 1), mask.long()</code></pre>
        </section>

        <section id="phase-3">
            <h2>Phase III: Training Dynamics</h2>
            <p><strong>Focus:</strong> Loss Functions & Optimization</p>
            <p>Training a pixel-wise classifier requires specific loss functions. I experimented with standard Cross-Entropy and Dice Loss to handle class imbalance (e.g., small objects in large backgrounds).</p>
            
            <div class="media-wrapper">
                <div class="diagram-placeholder">
                    
                </div>
                <div class="caption">Figure 2: Loss Convergence Analysis</div>
            </div>

            <ul>
                <li><strong>Optimization:</strong> Used the **Adam** optimizer with a learning rate of $1e-4$ and weight decay for regularization.</li>
                <li><strong>Metric:</strong> Evaluated using the **Dice Coefficient** (F1 Score), which measures the overlap between the predicted segmentation mask and the ground truth.</li>
            </ul>

            <span class="code-label">train.py / Optimization Step</span>
            <pre><code class="language-python">for epoch in range(epochs):
    net.train()
    for batch in dataloader:
        imgs = batch['image'].to(device)
        true_masks = batch['mask'].to(device)

        # 1. Forward Pass
        masks_pred = net(imgs)
        
        # 2. Compute Loss (Cross Entropy + Dice)
        loss = criterion(masks_pred, true_masks)
        
        # 3. Backpropagation
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</code></pre>
        </section>

        <section id="phase-4">
            <h2>Phase IV: Inference & Results</h2>
            <p><strong>Focus:</strong> Visualization & Mask Overlay</p>
            <p>The final model successfully delineates boundaries between classes. Below is a comparison of the raw input, the ground truth annotation, and the model's prediction.</p>

            <div class="media-wrapper">
                <div class="diagram-placeholder">
                    
                </div>
                <div class="caption">Figure 3: Inference Results (Input / Truth / Predicted)</div>
            </div>

            <ul>
                <li><strong>IoU Score:</strong> Achieved an Intersection over Union (IoU) of <strong>0.82</strong> on the test set.</li>
                <li><strong>Generalization:</strong> The model robustly handles variations in lighting and orientation due to the extensive augmentation pipeline.</li>
            </ul>
        </section>
        
        <footer>
            <div class="footer-content">
                <div>&copy; 2024 Object-Centric Pipeline</div>
                <div class="footer-links">
                    <a href="#">GitHub</a>
                    <a href="#">LinkedIn</a>
                    <a href="#">Email</a>
                </div>
            </div>
        </footer>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>

</body>
</html>
